<!DOCTYPE html>
<html>
<head>
<title>README.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="security-automation-framework-cli">Security Automation Framework CLI</h1>
<p>The MITRE Security Automation Framework (SAF) Command Line Interface (CLI) brings together applications, techniques, libraries, and tools developed by MITRE and the security community to streamline security automation for systems and DevOps pipelines</p>
<p>The SAF CLI is the successor to <a href="https://github.com/mitre/heimdall_tools">Heimdall Tools</a> and <a href="https://github.com/mitre/inspec_tools">InSpec Tools</a>.</p>
<h2 id="terminology">Terminology:</h2>
<ul>
<li>&quot;<a href="https://github.com/mitre/heimdall2">Heimdall</a>&quot; - Our visualizer for all security result data</li>
<li>&quot;<a href="https://saf.mitre.org/#/normalize">Heimdall Data Format (HDF)</a>&quot; - Our common data format to preserve and transform security data</li>
</ul>
<h2 id="installation">Installation:</h2>
<ul>
<li><a href="#installation-via-npm">Via NPM</a>
<ul>
<li><a href="#update-via-npm">Update via NPM</a></li>
</ul>
</li>
<li><a href="#installation-via-brew">Via Brew</a>
<ul>
<li><a href="#update-via-brew">Update via Brew</a></li>
</ul>
</li>
<li><a href="#installation-via-docker">Via Docker</a>
<ul>
<li><a href="#update-via-docker">Update via Docker</a></li>
</ul>
</li>
<li><a href="#installation-via-windows-installer">Via Windows Installer</a>
<ul>
<li><a href="#update-via-windows-installer">Update via Windows Installer</a></li>
</ul>
</li>
</ul>
<h2 id="usage">Usage</h2>
<h3 id="attest-hdf-data">Attest HDF Data</h3>
<ul>
<li><a href="#attest">Attest</a>
<ul>
<li><a href="#create-attestations">Create Attestations</a></li>
<li><a href="#apply-attestations">Apply Attestations</a></li>
</ul>
</li>
</ul>
<h3 id="convert-hdf-to-other-formats">Convert HDF to Other Formats</h3>
<ul>
<li><a href="#convert">Get Help with Convert</a></li>
<li><a href="#convert-from-hdf">Convert From HDF</a>
<ul>
<li><a href="#hdf-to-asff">HDF to ASFF</a></li>
<li><a href="#hdf-to-splunk">HDF to Splunk</a></li>
<li><a href="#hdf-to-xccdf-results">HDF to XCCDF Results</a></li>
<li><a href="#hdf-to-checklist">HDF to Checklist</a></li>
<li><a href="#hdf-to-csv">HDF to CSV</a></li>
<li><a href="#hdf-to-caat">HDF to CAAT</a></li>
<li><a href="#hdf-to-condensed-json">HDF to Condensed JSON</a></li>
</ul>
</li>
</ul>
<h3 id="convert-other-formats-to-hdf">Convert Other Formats to HDF</h3>
<ul>
<li><a href="#convert-to-hdf">Convert To HDF</a>
<ul>
<li><a href="#asff-to-hdf">ASFF to HDF</a></li>
<li><a href="#aws-config-to-hdf">AWS Config to HDF</a></li>
<li><a href="#burp-suite-to-hdf">Burp Suite to HDF</a></li>
<li><a href="#conveyor-to-hdf">Conveyor to HDF</a></li>
<li><a href="#checklist-to-hdf">Checklist to HDF</a></li>
<li><a href="#dbprotect-to-hdf">DBProtect to HDF</a></li>
<li><a href="#fortify-to-hdf">Fortify to HDF</a></li>
<li><a href="#gosec-to-hdf">GoSec to HDF</a></li>
<li><a href="#ion-channel-to-hdf">Ion Channel to HDF</a></li>
<li><a href="#jfrog-xray-to-hdf">JFrog Xray to HDF</a></li>
<li><a href="#tenable-nessus-to-hdf">Tenable Nessus to HDF</a></li>
<li><a href="#netsparker-to-hdf">Netsparker to HDF</a></li>
<li><a href="#nikto-to-hdf">Nikto to HDF</a></li>
<li><a href="#prisma-to-hdf">Prisma to HDF</a></li>
<li><a href="#prowler-to-hdf">Prowler to HDF</a></li>
<li><a href="#sarif-to-hdf">Sarif to HDF</a></li>
<li><a href="#scoutsuite-to-hdf">Scoutsuite to HDF</a></li>
<li><a href="#snyk-to-hdf">Snyk to HDF</a></li>
<li><a href="#sonarqube-to-hdf">SonarQube to HDF</a></li>
<li><a href="#splunk-to-hdf">Splunk to HDF</a></li>
<li><a href="#trivy-to-hdf">Trivy to HDF</a></li>
<li><a href="#twistlock-to-hdf">Twistlock to HDF</a></li>
<li><a href="#veracode-to-hdf">Veracode to HDF</a></li>
<li><a href="#xccdf-results-to-hdf">XCCDF Results to HDF</a></li>
<li><a href="#owasp-zap-to-hdf">OWASP ZAP to HDF</a></li>
</ul>
</li>
</ul>
<h3 id="other-useful-converters">Other Useful Converters</h3>
<ul>
<li><a href="#checklist-to-poam">Checklist to POA&amp;M</a></li>
</ul>
<h3 id="emasser-client">eMasser Client</h3>
<ul>
<li><a href="#emass-api-cli">eMASS API CLI</a></li>
</ul>
<h3 id="view-hdf-summaries-and-data">View HDF Summaries and Data</h3>
<ul>
<li><a href="#view">View</a>
<ul>
<li><a href="#heimdall">Heimdall</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</li>
</ul>
<h3 id="validate-hdf-thresholds">Validate HDF Thresholds</h3>
<ul>
<li><a href="#validate">Validate</a>
<ul>
<li><a href="#thresholds">Thresholds</a></li>
</ul>
</li>
</ul>
<h3 id="generate-data-reports-and-more">Generate Data Reports and More</h3>
<ul>
<li><a href="#generate">Generate</a>
<ul>
<li><a href="#delta">Delta</a></li>
<li><a href="#delta-supporting-options">Delta Supporting Commands</a></li>
<li><a href="#ckl-templates">CKL Templates</a></li>
<li><a href="#inspec-metadata">InSpec Metadata</a></li>
<li><a href="#thresholds-1">Thresholds</a></li>
<li><a href="#spreadsheet-csvxlsx-to-inspec">Spreadsheet (csv/xlsx) to InSpec</a></li>
<li><a href="#xccdf-benchmark-to-inspec-stub">XCCDF Benchmark to InSpec Stubs</a>
<ul>
<li><a href="#dod-stub-vs-cis-stub-formatting">DoD Stub vs CIS Stub Formatting</a></li>
</ul>
</li>
<li><a href="#mapping-files">Mapping Files</a></li>
</ul>
</li>
</ul>
<h3 id="enhance-and-supplement-hdf-data">Enhance and Supplement HDF Data</h3>
<ul>
<li><a href="#supplement">Supplement</a>
<ul>
<li><a href="#passthrough">Passthrough</a>
<ul>
<li><a href="#read">Read</a></li>
<li><a href="#write">Write</a></li>
</ul>
</li>
<li><a href="#target">Target</a>
<ul>
<li><a href="#read-1">Read</a></li>
<li><a href="#write-1">Write</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="license-and-authors">License and Authors</h3>
<ul>
<li><a href="#license-and-author">License and Author</a></li>
</ul>
<hr>
<h2 id="installation">Installation</h2>
<hr>
<h3 id="installation-via-npm">Installation via NPM</h3>
<p>The SAF CLI can be installed and kept up to date using <code>npm</code>, which is included with most versions of <a href="https://nodejs.org/en/">NodeJS</a>.</p>
<pre class="hljs"><code><div>npm install -g @mitre/saf
</div></code></pre>
<h4 id="update-via-npm">Update via NPM</h4>
<p>To update the SAF CLI with <code>npm</code>:</p>
<pre class="hljs"><code><div>npm update -g @mitre/saf
</div></code></pre>
<p><a href="#installation">top</a></p>
<hr>
<h3 id="installation-via-brew">Installation via Brew</h3>
<p>The SAF CLI can be installed and kept up to date using <code>brew</code>.</p>
<pre class="hljs"><code><div>brew install mitre/saf/saf-cli
</div></code></pre>
<h4 id="update-via-brew">Update via Brew</h4>
<p>To update the SAF CLI with <code>brew</code>:</p>
<pre class="hljs"><code><div>brew upgrade mitre/saf/saf-cli
</div></code></pre>
<p><a href="#installation">top</a></p>
<hr>
<h3 id="installation-via-docker">Installation via Docker</h3>
<p><strong>On Linux and Mac:</strong></p>
<p>The docker command below can be used to run the SAF CLI one time, where <code>arguments</code> contains the command and flags you want to run. For ex: <code>--version</code> or <code>view summary -i hdf-results.json</code>.</p>
<pre class="hljs"><code><div>docker run -it -v$(pwd):/share mitre/saf &lt;arguments&gt;
</div></code></pre>
<p>To run the SAF CLI with a persistent shell for one or more commands, use the following, then run each full command. For ex: <code>saf --version</code> or <code>saf view summary -i hdf-results.json</code>. You can change the entrypoint you wish to use. For example, run with <code>--entrypoint sh</code> to open in a shell terminal. If the specified entrypoint is not found, try using the path such as <code>--entrypoint /bin/bash</code>.</p>
<pre class="hljs"><code><div>docker run --rm -it --entrypoint bash -v$(pwd):/share mitre/saf
</div></code></pre>
<p><strong>On Windows:</strong></p>
<p>The docker command below can be used to run the SAF CLI one time, where <code>arguments</code> contains the command and flags you want to run. For ex: <code>--version</code> or <code>view summary -i hdf-results.json</code>.</p>
<pre class="hljs"><code><div>docker run -it -v%cd%:/share mitre/saf &lt;arguments&gt;
</div></code></pre>
<p>To run the SAF CLI with a persistent shell for one or more commands, use the following, then run each full command. For ex: <code>saf --version</code> or <code>saf view summary -i hdf-results.json</code>. You can change the entrypoint you wish to use. For example, run with <code>--entrypoint sh</code> to open in a shell terminal. If the specified entrypoint is not found, try using the path such as <code>--entrypoint /bin/bash</code>.</p>
<pre class="hljs"><code><div>docker run --rm -it --entrypoint sh -v%cd%:/share mitre/saf
</div></code></pre>
<p><strong>NOTE:</strong></p>
<p>Remember to use Docker CLI flags as necessary to run the various subcommands.</p>
<p>For example, to run the <code>emasser configure</code> subcommand, you need to pass in a volume that contains your certificates and where you can store the resultant .env.  Furthermore, you need to pass in flags for enabling the pseudo-TTY and interactivity.</p>
<pre class="hljs"><code><div>docker run -it -v &quot;$(pwd)&quot;:/share mitre/saf emasser configure
</div></code></pre>
<p>Other commands might not require the <code>-i</code> or <code>-t</code> flags and instead only need a bind-mounted volume, such as a file based <code>convert</code>.</p>
<pre class="hljs"><code><div>docker run --rm -v &quot;$(pwd)&quot;:/share mitre/saf convert -i test/sample_data/trivy/sample_input_report/trivy-image_golang-1.12-alpine_sample.json -o test.json
</div></code></pre>
<p>Other flags exist to open up network ports or pass through environment variables so make sure to use whichever ones are required to successfully run a command.</p>
<h4 id="update-via-docker">Update via Docker</h4>
<p>To update the SAF CLI with <code>docker</code>:</p>
<pre class="hljs"><code><div>docker pull mitre/saf:latest
</div></code></pre>
<p><a href="#installation">top</a></p>
<hr>
<h3 id="installation-via-windows-installer">Installation via Windows Installer</h3>
<p>To install the latest release of the SAF CLI on Windows, download and run the most recent installer for your system architecture from the <a href="https://github.com/mitre/saf/releases">Releases</a> 🌬️ page.</p>
<h4 id="update-via-windows-installer">Update via Windows Installer</h4>
<p>To update the SAF CLI on Windows, uninstall any existing version from your system and then download and run the most recent installer for your system architecture from the <a href="https://github.com/mitre/saf/releases">Releases</a> 🌬️ page.</p>
<p><a href="#installation">top</a></p>
<h2 id="usage">Usage</h2>
<hr>
<h3 id="attest">Attest</h3>
<p>Attest to 'Not Reviewed' controls: sometimes requirements can’t be tested automatically by security tools and hence require manual review, whereby someone interviews people and/or examines a system to confirm (i.e., attest as to) whether the control requirements have been satisfied.</p>
<h4 id="create-attestations">Create Attestations</h4>
<pre class="hljs"><code><div>attest create              Create attestation files for use with `saf attest apply`

USAGE
  $ saf attest create -o &lt;attestation-file&gt; [-i &lt;hdf-json&gt; -t &lt;json | xlsx | yml | yaml&gt;]

FLAGS
  -h, --help             Show CLI help.
  -i, --input=&lt;value&gt;    (optional) An input HDF file to search for controls
  -o, --output=&lt;value&gt;   (required) The output filename
  -t, --format=&lt;option&gt;  [default: json] (optional) The output file type
                         &lt;options: json|xlsx|yml|yaml&gt;

EXAMPLES
  $ saf attest create -o attestation.json -i hdf.json

  $ saf attest create -o attestation.xlsx -t xlsx
</div></code></pre>
<p><a href="#usage">top</a></p>
<h4 id="apply-attestations">Apply Attestations</h4>
<pre class="hljs"><code><div>attest apply              Apply one or more attestation files to one or more HDF results sets

USAGE
  $ saf attest apply -i &lt;input-hdf-json&gt;... &lt;attestation&gt;... -o &lt;output-hdf-path&gt;

FLAGS
  -h, --help              Show CLI help.
  -i, --input=&lt;value&gt;...  (required) Your input HDF and Attestation file(s)
  -o, --output=&lt;value&gt;    (required) Output file or folder (for multiple executions)

EXAMPLES
  $ saf attest apply -i hdf.json attestation.json -o new-hdf.json

  $ saf attest apply -i hdf1.json hdf2.json attestation.xlsx -o outputDir
</div></code></pre>
<p><a href="#usage">top</a></p>
<h3 id="convert">Convert</h3>
<p>Translating your data to and from Heimdall Data Format (HDF) is done using the <code>saf convert</code> command.</p>
<p>Want to Recommend or Help Develop a Converter? See <a href="https://github.com/mitre/saf/wiki/How-to-recommend-development-of-a-mapper">the wiki</a> 📰 on how to get started.</p>
<h3 id="convert-from-hdf">Convert From HDF</h3>
<h4 id="hdf-to-asff">HDF to ASFF</h4>
<p><em><strong>Note:</strong></em> Uploading findings into AWS Security hub requires configuration of the AWS CLI, see 👉 <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">the AWS documentation</a> or configuration of environment variables via Docker.</p>
<pre class="hljs"><code><div>convert hdf2asff              Translate a Heimdall Data Format JSON file into
                              AWS Security Findings Format JSON file(s) and/or
                              upload to AWS Security Hub
  USAGE
    $ saf convert hdf2asff -a &lt;account-id&gt; -r &lt;region&gt; -i &lt;hdf-scan-results-json&gt; -t &lt;target&gt; [-h] [-R] (-u [-I -C &lt;certificate&gt;] | [-o &lt;asff-output-folder&gt;])

  FLAGS
    -C, --certificate=&lt;certificate&gt;           Trusted signing certificate file
    -I, --insecure                            Disable SSL verification, this is insecure.
    -R, --specifyRegionAttribute              Manually specify the top-level `Region` attribute - SecurityHub
                                              populates this attribute automatically and prohibits one from
                                              updating it using `BatchImportFindings` or `BatchUpdateFindings`
    -a, --accountId=&lt;account-id&gt;              (required) AWS Account ID
    -h, --help                                Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;       (required) Input HDF JSON File
    -o, --output=&lt;asff-output-folder&gt;         Output ASFF JSON Folder
    -r, --region=&lt;region&gt;          (required) SecurityHub Region
    -t, --target=&lt;target&gt;          (required) Unique name for target to track findings across time
    -u, --upload                  Upload findings to AWS Security Hub
  EXAMPLES
    $ saf convert hdf2asff -i rhel7-scan_02032022A.json -a 123456789 -r us-east-1 -t rhel7_example_host -o rhel7.asff
    $ saf convert hdf2asff -i rds_mysql_i123456789scan_03042022A.json -a 987654321 -r us-west-1 -t Instance_i123456789 -u
    $ saf convert hdf2asff -i snyk_acme_project5_hdf_04052022A.json -a 2143658798 -r us-east-1 -t acme_project5 -o snyk_acme_project5 -u
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-splunk">HDF to Splunk</h4>
<p><strong>Notice</strong>: HDF to Splunk requires configuration on the Splunk server. See 👉 <a href="https://github.com/mitre/saf/wiki/Splunk-Configuration">Splunk Configuration</a>.</p>
<pre class="hljs"><code><div>convert hdf2splunk            Translate and upload a Heimdall Data Format JSON file into a Splunk server

  USAGE
    $ saf convert hdf2splunk -i &lt;hdf-scan-results-json&gt; -H &lt;host&gt; -I &lt;index&gt; [-h] [-P &lt;port&gt;] [-s http|https] [-u &lt;username&gt; | -t &lt;token&gt;] [-p &lt;password&gt;] [-L info|warn|debug|verbose]

  FLAGS
    -H, --host=&lt;host&gt;                       (required) Splunk Hostname or IP
    -I, --index=&lt;index&gt;                     (required) Splunk index to import HDF data into
    -L, --logLevel=&lt;option&gt;                 [default: info]
                                            &lt;options: info|warn|debug|verbose&gt;
    -P, --port=&lt;port&gt;                       [default: 8089] Splunk management port (also known as the Universal Forwarder port)
    -h, --help                              Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;     (required) Input HDF file
    -p, --password=&lt;password&gt;               Your Splunk password
    -s, --scheme=&lt;option&gt;                   [default: https] HTTP Scheme used for communication with splunk
                                            &lt;options: http|https&gt;
    -t, --token=&lt;token&gt;                     Your Splunk API Token
    -u, --username=&lt;username&gt;               Your Splunk username

  EXAMPLES
    $ saf convert hdf2splunk -i rhel7-results.json -H 127.0.0.1 -u admin -p Valid_password! -I hdf
    $ saf convert hdf2splunk -i rhel7-results.json -H 127.0.0.1 -t your.splunk.token -I hdf
</div></code></pre>
<p>For HDF Splunk Schema documentation visit 👉 <a href="https://github.com/mitre/heimdall2/blob/master/libs/hdf-converters/src/converters-from-hdf/splunk/Schemas.md#schemas">Heimdall converter schemas</a></p>
<p><strong>Previewing HDF Data Within Splunk:</strong></p>
<p>An example of a full raw search query:</p>
<pre class="hljs"><code><div>index="&lt;&lt;YOUR INDEX&gt;&gt;" meta.subtype=control | stats  <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">list</span>(meta.profile_sha256) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">first</span>(meta.status)  <span class="hljs-keyword">list</span>(meta.status)  <span class="hljs-keyword">list</span>(meta.is_baseline) <span class="hljs-keyword">values</span>(title) <span class="hljs-keyword">last</span>(code) <span class="hljs-keyword">list</span>(code) <span class="hljs-keyword">values</span>(<span class="hljs-keyword">desc</span>) <span class="hljs-keyword">values</span>(descriptions.*)  <span class="hljs-keyword">values</span>(<span class="hljs-keyword">id</span>) <span class="hljs-keyword">values</span>(impact) <span class="hljs-keyword">list</span>(refs{}.*) <span class="hljs-keyword">list</span>(results{}.*) <span class="hljs-keyword">list</span>(source_location{}.*) <span class="hljs-keyword">values</span>(tags.*)  <span class="hljs-keyword">by</span> meta.guid <span class="hljs-keyword">id</span>
| <span class="hljs-keyword">join</span>  meta.guid
    [<span class="hljs-keyword">search</span> <span class="hljs-keyword">index</span>=<span class="hljs-string">"&lt;&lt;YOUR INDEX&gt;&gt;"</span>  meta.subtype=header | stats <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">list</span>(statistics.duration)  <span class="hljs-keyword">list</span>(platform.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">version</span>)  <span class="hljs-keyword">by</span> meta.guid]
| <span class="hljs-keyword">join</span> meta.guid
    [<span class="hljs-keyword">search</span> <span class="hljs-keyword">index</span>=<span class="hljs-string">"&lt;&lt;YOUR INDEX&gt;&gt;"</span>  meta.subtype=profile | stats <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">list</span>(meta.profile_sha256) <span class="hljs-keyword">list</span>(meta.is_baseline)  <span class="hljs-keyword">last</span>(summary) <span class="hljs-keyword">list</span>(summary) <span class="hljs-keyword">list</span>(sha256) <span class="hljs-keyword">list</span>(supports{}.*) <span class="hljs-keyword">last</span>(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">list</span>(copyright) <span class="hljs-keyword">list</span>(maintainer) <span class="hljs-keyword">list</span>(copyright_email) <span class="hljs-keyword">last</span>(<span class="hljs-keyword">version</span>) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">version</span>) <span class="hljs-keyword">list</span>(license) <span class="hljs-keyword">list</span>(title) <span class="hljs-keyword">list</span>(parent_profile) <span class="hljs-keyword">list</span>(depends{}.*) <span class="hljs-keyword">list</span>(controls{}.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">attributes</span>{}.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">status</span>) <span class="hljs-keyword">by</span> meta.guid]

</div></code></pre>
<p>An example of a formatted table search query:</p>
<pre class="hljs"><code><div>index="&lt;&lt;YOUR INDEX&gt;&gt;" meta.subtype=control | stats  <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">list</span>(meta.profile_sha256) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">first</span>(meta.status)  <span class="hljs-keyword">list</span>(meta.status)  <span class="hljs-keyword">list</span>(meta.is_baseline) <span class="hljs-keyword">values</span>(title) <span class="hljs-keyword">last</span>(code) <span class="hljs-keyword">list</span>(code) <span class="hljs-keyword">values</span>(<span class="hljs-keyword">desc</span>) <span class="hljs-keyword">values</span>(descriptions.*)  <span class="hljs-keyword">values</span>(<span class="hljs-keyword">id</span>) <span class="hljs-keyword">values</span>(impact) <span class="hljs-keyword">list</span>(refs{}.*) <span class="hljs-keyword">list</span>(results{}.*) <span class="hljs-keyword">list</span>(source_location{}.*) <span class="hljs-keyword">values</span>(tags.*)  <span class="hljs-keyword">by</span> meta.guid <span class="hljs-keyword">id</span>
| <span class="hljs-keyword">join</span>  meta.guid
    [<span class="hljs-keyword">search</span> <span class="hljs-keyword">index</span>=<span class="hljs-string">"&lt;&lt;YOUR INDEX&gt;&gt;"</span>  meta.subtype=header | stats <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">list</span>(statistics.duration)  <span class="hljs-keyword">list</span>(platform.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">version</span>)  <span class="hljs-keyword">by</span> meta.guid]
| <span class="hljs-keyword">join</span> meta.guid
    [<span class="hljs-keyword">search</span> <span class="hljs-keyword">index</span>=<span class="hljs-string">"&lt;&lt;YOUR INDEX&gt;&gt;"</span>  meta.subtype=profile | stats <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">values</span>(meta.hdf_splunk_schema) <span class="hljs-keyword">list</span>(meta.profile_sha256) <span class="hljs-keyword">list</span>(meta.is_baseline)  <span class="hljs-keyword">last</span>(summary) <span class="hljs-keyword">list</span>(summary) <span class="hljs-keyword">list</span>(sha256) <span class="hljs-keyword">list</span>(supports{}.*) <span class="hljs-keyword">last</span>(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">list</span>(copyright) <span class="hljs-keyword">list</span>(maintainer) <span class="hljs-keyword">list</span>(copyright_email) <span class="hljs-keyword">last</span>(<span class="hljs-keyword">version</span>) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">version</span>) <span class="hljs-keyword">list</span>(license) <span class="hljs-keyword">list</span>(title) <span class="hljs-keyword">list</span>(parent_profile) <span class="hljs-keyword">list</span>(depends{}.*) <span class="hljs-keyword">list</span>(controls{}.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">attributes</span>{}.*) <span class="hljs-keyword">list</span>(<span class="hljs-keyword">status</span>) <span class="hljs-keyword">by</span> meta.guid]
| <span class="hljs-keyword">rename</span> <span class="hljs-keyword">values</span>(meta.filename) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Results Set"</span>, <span class="hljs-keyword">values</span>(meta.filetype) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Scan Type"</span>, <span class="hljs-keyword">list</span>(statistics.duration) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Scan Duration"</span>, <span class="hljs-keyword">first</span>(meta.status) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Control Status"</span>, <span class="hljs-keyword">list</span>(results{}.status) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Test(s) Status"</span>, <span class="hljs-keyword">id</span> <span class="hljs-keyword">AS</span> <span class="hljs-string">"ID"</span>, <span class="hljs-keyword">values</span>(title) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Title"</span>, <span class="hljs-keyword">values</span>(<span class="hljs-keyword">desc</span>) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Description"</span>, <span class="hljs-keyword">values</span>(impact) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Impact"</span>, <span class="hljs-keyword">last</span>(code) <span class="hljs-keyword">AS</span> Code, <span class="hljs-keyword">values</span>(descriptions.check) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Check"</span>, <span class="hljs-keyword">values</span>(descriptions.fix) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Fix"</span>, <span class="hljs-keyword">values</span>(tags.cci{}) <span class="hljs-keyword">AS</span> <span class="hljs-string">"CCI IDs"</span>, <span class="hljs-keyword">list</span>(results{}.code_desc) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Results Description"</span>,  <span class="hljs-keyword">list</span>(results{}.skip_message) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Results Skip Message (if applicable)"</span>, <span class="hljs-keyword">values</span>(tags.nist{}) <span class="hljs-keyword">AS</span> <span class="hljs-string">"NIST SP 800-53 Controls"</span>, <span class="hljs-keyword">last</span>(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Scan (Profile) Name"</span>, <span class="hljs-keyword">last</span>(summary) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Scan (Profile) Summary"</span>, <span class="hljs-keyword">last</span>(<span class="hljs-keyword">version</span>) <span class="hljs-keyword">AS</span> <span class="hljs-string">"Scan (Profile) Version"</span>
| <span class="hljs-keyword">table</span> meta.guid <span class="hljs-string">"Results Set"</span> <span class="hljs-string">"Scan Type"</span> <span class="hljs-string">"Scan (Profile) Name"</span> <span class="hljs-keyword">ID</span> <span class="hljs-string">"NIST SP 800-53 Controls"</span> Title <span class="hljs-string">"Control Status"</span> <span class="hljs-string">"Test(s) Status"</span> <span class="hljs-string">"Results Description"</span> <span class="hljs-string">"Results Skip Message (if applicable)"</span>  Description Impact Severity  <span class="hljs-keyword">Check</span> Fix <span class="hljs-string">"CCI IDs"</span> Code <span class="hljs-string">"Scan Duration"</span> <span class="hljs-string">"Scan (Profile) Summary"</span> <span class="hljs-string">"Scan (Profile) Version"</span>
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-xccdf-results">HDF to XCCDF Results</h4>
<pre class="hljs"><code><div>convert hdf2xccdf             Translate an HDF file into an XCCDF XML file

  USAGE
    $ saf convert hdf2xccdf -i &lt;hdf-scan-results-json&gt; -o &lt;output-xccdf-xml&gt; [-h]

  FLAGS
    -h, --help                              Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;     (required) Input HDF file
    -o, --output=&lt;output-xccdf-xml&gt;         (required) Output XCCDF XML file

  EXAMPLES
    $ saf convert hdf2xccdf -i hdf_input.json -o xccdf-results.xml
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-checklist">HDF to Checklist</h4>
<pre class="hljs"><code><div>convert hdf2ckl               Translate a Heimdall Data Format JSON file into a
                              DISA checklist file

  USAGE
    $ saf convert hdf2ckl -i &lt;hdf-scan-results-json&gt; -o &lt;output-ckl&gt; [-h] [-m &lt;metadata&gt;] [-H &lt;hostname&gt;] [-F &lt;fqdn&gt;] [-M &lt;mac-address&gt;] [-I &lt;ip-address&gt;]

  FLAGS
    -F, --fqdn=&lt;fqdn&gt;                       FQDN for CKL metadata
    -H, --hostname=&lt;hostname&gt;               Hostname for CKL metadata
    -I, --ip=&lt;ip-address&gt;                   IP address for CKL metadata
    -M, --mac=&lt;mac-address&gt;                 MAC address for CKL metadata
    -h, --help                              Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;     (required) Input HDF file
    -m, --metadata=&lt;metadata&gt;               Metadata JSON file, generate one with &quot;saf generate ckl_metadata&quot;
    -o, --output=&lt;output-ckl&gt;               (required) Output CKL file

  EXAMPLES
    $ saf convert hdf2ckl -i rhel7-results.json -o rhel7.ckl --fqdn reverseproxy.example.org --hostname reverseproxy --ip 10.0.0.3 --mac 12:34:56:78:90
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-csv">HDF to CSV</h4>
<pre class="hljs"><code><div>convert hdf2csv               Translate a Heimdall Data Format JSON file into a
                              Comma Separated Values (CSV) file

  USAGE
    $ saf convert hdf2csv -i &lt;hdf-scan-results-json&gt; -o &lt;output-csv&gt; [-h] [-f &lt;csv-fields&gt;] [-t]

  FLAGS
    -f, --fields=&lt;csv-fields&gt;               [default: All Fields] Fields to include in output CSV, separated by commas
    -h, --help                              Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;     (required) Input HDF file
    -o, --output=&lt;output-csv&gt;               (required) Output CSV file
    -t, --noTruncate                        Do not truncate fields longer than 32,767 characters (the cell limit in Excel)

  EXAMPLES
    $ saf convert hdf2csv -i rhel7-results.json -o rhel7.csv --fields &quot;Results Set,Status,ID,Title,Severity&quot;
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-caat">HDF to CAAT</h4>
<pre class="hljs"><code><div>convert hdf2caat              Translate a Heimdall Data Format JSON file into a
                              Compliance Assessment and Audit Tracking (CAAT) XLSX file

  USAGE
    $ saf convert hdf2caat -i &lt;hdf-scan-results-json&gt;... -o &lt;output-caat-xlsx&gt; [-h]

  FLAGS
    -h, --help              Show CLI help.
    -i, --input=&lt;value&gt;...  (required) Input HDF JSON file
    -o, --output=&lt;value&gt;    (required) Output CAAT XLSX file

  EXAMPLES
    $ saf convert hdf2caat -i hdf_input.json -o caat-results.xlsx
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<h4 id="hdf-to-condensed-json">HDF to Condensed JSON</h4>
<pre class="hljs"><code><div>convert hdf2condensed         Condensed format used by some community members
                              to pre-process data for elasticsearch and custom dashboards

  USAGE
    $ saf convert hdf2condensed -i &lt;hdf-scan-results-json&gt; -o &lt;condensed-json&gt; [-h]

  FLAGS
    -h, --help            Show CLI help.
    -i, --input=&lt;hdf-scan-results-json&gt;     (required) Input HDF file
    -o, --output=&lt;condensed-json&gt;           (required) Output condensed JSON file

  EXAMPLES
    $ saf convert hdf2condensed -i rhel7-results.json -o rhel7-condensed.json
</div></code></pre>
<p><a href="#convert-hdf-to-other-formats">top</a></p>
<hr>
<h3 id="convert-to-hdf">Convert To HDF</h3>
<h4 id="asff-to-hdf">ASFF to HDF</h4>
<table>
<thead>
<tr>
<th>Output</th>
<th>Use</th>
<th>Command</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASFF json</td>
<td>All the findings that will be fed into the mapper</td>
<td>aws securityhub get-findings &gt; asff.json</td>
</tr>
<tr>
<td>AWS SecurityHub enabled standards json</td>
<td>Get all the enabled standards so you can get their identifiers</td>
<td>aws securityhub get-enabled-standards &gt; asff_standards.json</td>
</tr>
<tr>
<td>AWS SecurityHub standard controls json</td>
<td>Get all the controls for a standard that will be fed into the mapper</td>
<td>aws securityhub describe-standards-controls --standards-subscription-arn &quot;arn:aws:securityhub:us-east-1:123456789123:subscription/cis-aws-foundations-benchmark/v/1.2.0&quot; &gt; asff_cis_standard.json</td>
</tr>
</tbody>
</table>
<pre class="hljs"><code><div>convert asff2hdf              Translate a AWS Security Finding Format JSON into a
                              Heimdall Data Format JSON file(s)
  USAGE
    $ saf convert asff2hdf -o &lt;hdf-output-folder&gt; [-h] (-i &lt;asff-json&gt; [--securityhub &lt;standard-json&gt;...] | -a -r &lt;region&gt; [-I | -C &lt;certificate&gt;] [-t &lt;target&gt;...]) [-L info|warn|debug|verbose]

  FLAGS
    -C, --certificate=&lt;certificate&gt;       Trusted signing certificate file
    -I, --insecure                        Disable SSL verification, this is insecure.
    -L, --logLevel=&lt;option&gt;               [default: info]
                                          &lt;options: info|warn|debug|verbose&gt;
    -a, --aws                             Pull findings from AWS Security Hub
    -h, --help                            Show CLI help.
    -i, --input=&lt;asff-json&gt;               Input ASFF JSON file
    -o, --output=&lt;hdf-output-folder&gt;      (required) Output HDF JSON folder
    -r, --region=&lt;region&gt;                 Security Hub region to pull findings from
    -t, --target=&lt;target&gt;...              Target ID(s) to pull from Security Hub (maximum 10), leave blank for non-HDF findings
    --securityhub=&lt;standard-json&gt;...      Additional input files to provide context that an ASFF file needs
                                          such as the CIS AWS Foundations or AWS Foundational Security Best
                                          Practices documents (in ASFF compliant JSON form)

  EXAMPLES
    $ saf convert asff2hdf -i asff-findings.json -o output-folder-name
    $ saf convert asff2hdf -i asff-findings.json --securityhub standard-1.json standard-2.json -o output-folder-name
    $ saf convert asff2hdf --aws -o out -r us-west-2 --target rhel7
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="aws-config-to-hdf">AWS Config to HDF</h4>
<p><em><strong>Note:</strong></em> Pulling AWS Config results data requires configuration of the AWS CLI, see 👉 <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">the AWS documentation</a> or configuration of environment variables via Docker.</p>
<pre class="hljs"><code><div>convert aws_config2hdf        Pull Configuration findings from AWS Config and convert
                              into a Heimdall Data Format JSON file
  USAGE
    $ saf convert aws_config2hdf -r &lt;region&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-a &lt;access-key-id&gt;] [-s &lt;secret-access-key&gt;] [-t &lt;session-token&gt;] [-i]

  FLAGS
    -a, --accessKeyId=&lt;access-key-id&gt;           Access key ID
    -h, --help                                  Show CLI help.
    -i, --insecure                              Disable SSL verification, this is insecure.
    -o, --output=&lt;hdf-scan-results-json&gt;        (required) Output HDF JSON File
    -r, --region=&lt;region&gt;                       (required) Region to pull findings from
    -s, --secretAccessKey=&lt;secret-access-key&gt;   Secret access key
    -t, --sessionToken=&lt;session-token&gt;          Session token

  EXAMPLES
    $ saf convert aws_config2hdf -a ABCDEFGHIJKLMNOPQRSTUV -s +4NOT39A48REAL93SECRET934 -r us-east-1 -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="burp-suite-to-hdf">Burp Suite to HDF</h4>
<pre class="hljs"><code><div>convert burpsuite2hdf         Translate a BurpSuite Pro XML file into a Heimdall
                              Data Format JSON file
  USAGE
    $ saf convert burpsuite2hdf -i &lt;burpsuite-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                              Show CLI help.
    -i, --input=&lt;burpsuite-xml&gt;             (required) Input Burpsuite Pro XML File
    -o, --output=&lt;hdf-scan-results-json&gt;    (required) Output HDF JSON File
    -w, --with-raw                          Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert burpsuite2hdf -i burpsuite_results.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="conveyor-to-hdf">Conveyor to HDF</h4>
<pre class="hljs"><code><div>convert conveyor2hdf          Translate a conveyor JSON file into a Heimdall Data
                              Format JSON file
  USAGE
    $ saf convert conveyor2hdf -i &lt;conveyor-json&gt; -o &lt;hdf-scan-results-dir&gt; [-h]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;conveyor-json&gt;            (required) Input Conveyor JSON File
    -o, --output=&lt;hdf-scan-results-dir&gt;  (required) Output HDF Folder

  EXAMPLES
    $ saf convert conveyor2hdf -i conveyor_results.json -o output-hdf-dirs.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="checklist-to-hdf">Checklist to HDF</h4>
<pre class="hljs"><code><div>convert ckl2hdf               Translate a DISA Checklist XML file into a Heimdall Data 
                              Format JSON file
  USAGE
    $ saf convert ckl2hdf -i &lt;ckl-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-s] [-w]

  FLAGS
    -h, --help            Show CLI help.
    -i, --input=&lt;value&gt;   (required) Input Checklist XML File
    -o, --output=&lt;value&gt;  (required) Output HDF JSON File
    -w, --with-raw        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert ckl2hdf -i ckl_results.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="dbprotect-to-hdf">DBProtect to HDF</h4>
<pre class="hljs"><code><div>convert dbprotect2hdf         Translate a DBProtect report in &quot;Check Results
                              Details&quot; XML format into a Heimdall Data Format JSON file
  USAGE
    $ saf convert dbprotect2hdf -i &lt;dbprotect-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;dbprotect-xml&gt;           (required) 'Check Results Details' XML File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert dbprotect2hdf -i check_results_details_report.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="fortify-to-hdf">Fortify to HDF</h4>
<pre class="hljs"><code><div>convert fortify2hdf           Translate a Fortify results FVDL file into a Heimdall
                              Data Format JSON file; the FVDL file is an XML that can be
                              extracted from the Fortify FPR project file using standard
                              file compression tools
  USAGE
    $ saf convert fortify2hdf -i &lt;fortify-fvdl&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;fortify-fvdl&gt;            (required) Input FVDL File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert fortify2hdf -i audit.fvdl -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="gosec-to-hdf">GoSec to HDF</h4>
<pre class="hljs"><code><div>convert gosec2hdf             Translate a GoSec (Golang Security Checker) results file
                              into a Heimdall Data Format JSON file
  USAGE
    $ saf convert gosec2hdf -i &lt;gosec-json&gt; -o &lt;hdf-scan-results-json&gt; [-h]

  FLAGS
    -h, --help            Show CLI help.
    -i, --input=&lt;value&gt;   (required) Input GoSec Results JSON File
    -o, --output=&lt;value&gt;  (required) Output HDF JSON File

  EXAMPLES
    $ saf convert gosec2hdf -i gosec_results.json -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="ion-channel-to-hdf">Ion Channel to HDF</h4>
<pre class="hljs"><code><div>convert ionchannel2hdf        Pull and translate SBOM data from Ion Channel
                              into Heimdall Data Format
  USAGE
    $ saf convert ionchannel2hdf -o &lt;hdf-output-folder&gt; [-h] (-i &lt;ionchannel-json&gt;... | -a &lt;api-key&gt; -t &lt;team-name&gt; [--raw ] [-p &lt;project&gt;...] [-A ]) [-L info|warn|debug|verbose]

  FLAGS
    -A, --allProjects                   Pull all projects available within your team
    -L, --logLevel=&lt;option&gt;             [default: info]
                                        &lt;options: info|warn|debug|verbose&gt;
    -a, --apiKey=&lt;api-key&gt;              API Key from Ion Channel user settings
    -h, --help                          Show CLI help.
    -i, --input=&lt;ionchannel-json&gt;...    Input IonChannel JSON file
    -o, --output=&lt;hdf-output-folder&gt;    (required) Output JSON folder
    -p, --project=&lt;project&gt;...          The name of the project(s) you would like to pull
    -t, --teamName=&lt;team-name&gt;          Your team name that contains the project(s) you would like to pull data from
    --raw                               Output Ion Channel raw data
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="jfrog-xray-to-hdf">JFrog Xray to HDF</h4>
<pre class="hljs"><code><div>convert jfrog_xray2hdf        Translate a JFrog Xray results JSON file into a
                              Heimdall Data Format JSON file
  USAGE
    $ saf convert jfrog_xray2hdf -i &lt;jfrog-xray-json&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;jfrog-xray-json&gt;         (required) Input JFrog JSON File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert jfrog_xray2hdf -i xray_results.json -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="tenable-nessus-to-hdf">Tenable Nessus to HDF</h4>
<pre class="hljs"><code><div>convert nessus2hdf            Translate a Nessus XML results file into a Heimdall
                              Data Format JSON file. The current iteration maps all
                              plugin families except for 'Policy Compliance'
                              A separate HDF JSON is generated for each host reported in the Nessus Report.
  USAGE
    $ saf convert nessus2hdf -i &lt;nessus-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;nessus-xml&gt;              (required) Input Nessus XML File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert nessus2hdf -i nessus_results.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="netsparker-to-hdf">Netsparker to HDF</h4>
<pre class="hljs"><code><div>convert netsparker2hdf        Translate a Netsparker XML results file into a
                              Heimdall Data Format JSON file. The current
                              iteration only works with Netsparker Enterprise
                              Vulnerabilities Scan.
  USAGE
    $ saf convert netsparker2hdf -i &lt;netsparker-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;netsparker-xml&gt;          (required) Input Netsparker XML File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert netsparker2hdf -i netsparker_results.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="nikto-to-hdf">Nikto to HDF</h4>
<pre class="hljs"><code><div>convert nikto2hdf             Translate a Nikto results JSON file into a Heimdall
                              Data Format JSON file.
                              Note: Currently this mapper only supports single
                              target Nikto Scans
  USAGE
    $ saf convert nikto2hdf -i &lt;nikto-json&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;nikto-json&gt;              (required) Input Niktop Results JSON File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert nikto2hdf -i nikto-results.json -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="prisma-to-hdf">Prisma to HDF</h4>
<pre class="hljs"><code><div>convert prisma2hdf            Translate a Prisma Cloud Scan Report CSV file into
                              Heimdall Data Format JSON files
  USAGE
    $ saf convert prisma2hdf -i &lt;prisma-cloud-csv&gt; -o &lt;hdf-output-folder&gt; [-h]

  FLAGS
    -h, --help                        Show CLI help.
    -i, --input=&lt;prisma-cloud-csv&gt;    (required) Prisma Cloud Scan Report CSV
    -o, --output=&lt;hdf-output-folder&gt;  (required) Output HDF JSON file

  EXAMPLES
    $ saf convert prisma2hdf -i prismacloud-report.csv -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="prowler-to-hdf">Prowler to HDF</h4>
<pre class="hljs"><code><div>convert prowler2hdf           Translate a Prowler-derived AWS Security Finding
                              Format results from JSONL
                              into a Heimdall Data Format JSON file
  USAGE
    $ saf convert prowler2hdf -i &lt;prowler-finding-json&gt; -o &lt;hdf-output-folder&gt; [-h]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;prowler-finding-json&gt;    (required) Input Prowler ASFF JSON File
    -o, --output=&lt;hdf-output-folder&gt;      (required) Output HDF JSON Folder

  EXAMPLES
    $ saf convert prowler2hdf -i prowler-asff.json -o output-folder
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="sarif-to-hdf">Sarif to HDF</h4>
<pre class="hljs"><code><div>convert sarif2hdf             Translate a SARIF JSON file into a Heimdall Data
                              Format JSON file
  USAGE
    $ saf convert sarif2hdf -i &lt;sarif-json&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;sarif-json&gt;              (required) Input SARIF JSON File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  DESCRIPTION
    SARIF level to HDF impact Mapping:
    SARIF level error -&gt; HDF impact 0.7
    SARIF level warning -&gt; HDF impact 0.5
    SARIF level note -&gt; HDF impact 0.3
    SARIF level none -&gt; HDF impact 0.1
    SARIF level not provided -&gt; HDF impact 0.1 as default

  EXAMPLES
    $ saf convert sarif2hdf -i sarif-results.json -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="scoutsuite-to-hdf">Scoutsuite to HDF</h4>
<pre class="hljs"><code><div>convert scoutsuite2hdf        Translate a ScoutSuite results from a Javascript
                              object into a Heimdall Data Format JSON file

                              Note: Currently this mapper only supports AWS
  USAGE
    $ saf convert scoutsuite2hdf -i &lt;scoutsuite-results-js&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;scoutsuite-results-js&gt;   (required) Input ScoutSuite Results JS File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert scoutsuite2hdf -i scoutsuite-results.js -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="snyk-to-hdf">Snyk to HDF</h4>
<pre class="hljs"><code><div>convert snyk2hdf              Translate a Snyk results JSON file into a Heimdall
                              Data Format JSON file
                              A separate HDF JSON is generated for each project
                              reported in the Snyk Report
  USAGE
    $ saf convert snyk2hdf -i &lt;snyk-json&gt; -o &lt;hdf-scan-results-json&gt; [-h]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;snyk-json&gt;               (required) Input Snyk Results JSON File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File

  EXAMPLES
    $ saf convert snyk2hdf -i snyk_results.json -o output-file-prefix
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="sonarqube-to-hdf">SonarQube to HDF</h4>
<pre class="hljs"><code><div>convert sonarqube2hdf         Pull SonarQube vulnerabilities for the specified
                              project name and optional branch or pull/merge
                              request ID name from an API and convert into a
                              Heimdall Data Format JSON file
  USAGE
    $ saf convert sonarqube2hdf -n &lt;sonar-project-key&gt; -u &lt;http://your.sonar.instance:9000&gt; -a &lt;your-sonar-api-key&gt; [ -b &lt;target-branch&gt; | -p &lt;pull-request-id&gt; ] -o &lt;hdf-scan-results-json&gt;

  FLAGS
    -a, --auth=&lt;your-sonar-api-key&gt;               (required) SonarQube API Key
    -b, --branch=&lt;target-branch&gt;                  Requires Sonarqube Developer Edition or above
    -h, --help                                    Show CLI help.
    -n, --projectKey=&lt;sonar-project-key&gt;          (required) SonarQube Project Key
    -o, --output=&lt;hdf-scan-results-json&gt;          (required) Output HDF JSON File
    -p, --pullRequestID=&lt;pull-request-id&gt;         Requires Sonarqube Developer Edition or above
    -u, --url=&lt;http://your.sonar.instance:9000&gt;   (required) SonarQube Base URL (excluding '/api')

  EXAMPLES
    $ saf convert sonarqube2hdf -n sonar_project_key -u http://sonar:9000 --auth abcdefg -p 123 -o scan_results.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="splunk-to-hdf">Splunk to HDF</h4>
<pre class="hljs"><code><div>convert splunk2hdf            Pull HDF data from your Splunk instance back into an HDF file

  USAGE
    $ saf splunk2hdf -H &lt;host&gt; -I &lt;index&gt; [-h] [-P &lt;port&gt;] [-s http|https] (-u &lt;username&gt; -p &lt;password&gt; | -t &lt;token&gt;) [-L info|warn|debug|verbose] [-i &lt;filename/GUID&gt;... -o &lt;hdf-output-folder&gt;]

  FLAGS
    -H, --host=&lt;host&gt;                   (required) Splunk Hostname or IP
    -I, --index=&lt;index&gt;                 (required) Splunk index to query HDF data from
    -L, --logLevel=&lt;option&gt;             [default: info]
                                        &lt;options: info|warn|debug|verbose&gt;
    -P, --port=&lt;port&gt;                   [default: 8089] Splunk management port (also known as the Universal Forwarder port)
    -h, --help                          Show CLI help.
    -i, --input=&lt;filename/GUID&gt;...      GUID(s) or Filename(s) of files from Splunk to convert
    -o, --output=&lt;hdf-output-folder&gt;    Output HDF JSON Folder
    -p, --password=&lt;password&gt;           Your Splunk password
    -s, --scheme=&lt;option&gt;               [default: https] HTTP Scheme used for communication with splunk
                                        &lt;options: http|https&gt;
    -t, --token=&lt;token&gt;                 Your Splunk API Token
    -u, --username=&lt;username&gt;           Your Splunk username

  EXAMPLES
    $ saf convert splunk2hdf -H 127.0.0.1 -u admin -p Valid_password! -I hdf -i some-file-in-your-splunk-instance.json -i yBNxQsE1mi4f3mkjtpap5YxNTttpeG -o output-folder
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="trivy-to-hdf">Trivy to HDF</h4>
<pre class="hljs"><code><div>convert trivy2hdf             Translate a Trivy-derived AWS Security Finding
                              Format results from JSONL
                              into a Heimdall Data Format JSON file
  USAGE
    $ saf convert trivy2hdf -i &lt;trivy-finding-json&gt; -o &lt;hdf-output-folder&gt;

  FLAGS
    -h, --help                        Show CLI help.
    -i, --input=&lt;trivy-finding-json&gt;  (required) Input Trivy ASFF JSON File
    -o, --output=&lt;hdf-output-folder&gt;  (required) Output HDF JSON Folder

  DESCRIPTION
    Note: Currently this mapper only supports the results of Trivy's `image`
    subcommand (featuring the CVE findings) while using the ASFF template format
    (which comes bundled with the repo).  An example call to Trivy to get this
    type of file looks as follows:
    AWS_REGION=us-east-1 AWS_ACCOUNT_ID=123456789012 trivy image --no-progress --format template --template &quot;@/absolute_path_to/git_clone_of/trivy/contrib/asff.tpl&quot; -o trivy_asff.json golang:1.12-alpine

  EXAMPLES
    $ saf convert trivy2hdf -i trivy-asff.json -o output-folder
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="twistlock-to-hdf">Twistlock to HDF</h4>
<pre class="hljs"><code><div>convert twistlock2hdf         Translate a Twistlock CLI output file into an HDF results set

  USAGE
    $ saf convert twistlock2hdf -i &lt;twistlock-json&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;twistlock-json&gt;          (required) Input Twistlock file
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert twistlock2hdf -i twistlock.json -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="veracode-to-hdf">Veracode to HDF</h4>
<pre class="hljs"><code><div>convert veracode2hdf          Translate a Veracode XML file into a Heimdall Data
                              Format JSON file
  USAGE
    $ saf convert veracode2hdf -i &lt;veracode-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;veracode-xml&gt;            (required) Input Veracode XML File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File

  EXAMPLES
    $ saf convert veracode2hdf -i veracode_results.xml -o output-hdf-name.json
</div></code></pre>
<h4 id="xccdf-results-to-hdf">XCCDF Results to HDF</h4>
<p><em><strong>Note:</strong></em> <code>xccdf_results2hdf</code> only supports native OpenSCAP and SCC output.</p>
<pre class="hljs"><code><div>
[top](#convert-other-formats-to-hdf)
convert xccdf_results2hdf     Translate a SCAP client XCCDF-Results XML report
                              to a Heimdall Data Format JSON file
  USAGE
    $ saf convert xccdf_results2hdf -i &lt;xccdf-results-xml&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help            Show CLI help.
    -i, --input=&lt;xccdf-results-xml&gt;       (required) Input XCCDF Results XML File
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert xccdf_results2hdf -i results-xccdf.xml -o output-hdf-name.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<h4 id="owasp-zap-to-hdf">OWASP ZAP to HDF</h4>
<pre class="hljs"><code><div>convert zap2hdf               Translate a OWASP ZAP results JSON to a Heimdall Data Format JSON file

  USAGE
    $ saf convert zap2hdf -i &lt;zap-json&gt; -n &lt;target-site-name&gt; -o &lt;hdf-scan-results-json&gt; [-h] [-w]

  FLAGS
    -h, --help                            Show CLI help.
    -i, --input=&lt;zap-json&gt;                (required) Input OWASP Zap Results JSON File
    -n, --name=&lt;target-site-name&gt;         (required) Target Site Name
    -o, --output=&lt;hdf-scan-results-json&gt;  (required) Output HDF JSON File
    -w, --with-raw                        Include raw input file in HDF JSON file

  EXAMPLES
    $ saf convert zap2hdf -i zap_results.json -n mitre.org -o scan_results.json
</div></code></pre>
<p><a href="#convert-other-formats-to-hdf">top</a></p>
<hr>
<h3 id="other-useful-converters">Other Useful Converters</h3>
<h4 id="checklist-to-poam">Checklist to POA&amp;M</h4>
<p>Note: The included CCI to NIST Mappings are the extracted from NIST.gov, for mappings specific to eMASS use <a href="https://github.com/mitre/ckl2POAM/blob/main/resources/cci2nist.json">this</a> file instead).</p>
<pre class="hljs"><code><div>convert ckl2POAM              Translate DISA Checklist CKL file(s) to POA&amp;M files

  USAGE
    $ saf convert ckl2POAM -i &lt;disa-checklist&gt;... -o &lt;poam-output-folder&gt; [-h] [-O &lt;office/org&gt;] [-d &lt;device-name&gt;] [-s &lt;num-rows&gt;]

  FLAGS
    -O, --officeOrg=&lt;office/org&gt;          Default value for Office/org (prompts for each file if not set)
    -d, --deviceName=&lt;device-name&gt;        Name of target device (prompts for each file if not set)
    -h, --help                            Show CLI help.
    -i, --input=&lt;disa-checklist&gt;...       (required) Path to the DISA Checklist File(s)
    -o, --output=&lt;poam-output-folder&gt;     (required) Path to output PO&amp;M File(s)
    -s, --rowsToSkip=&lt;num-rows&gt;           [default: 4] Rows to leave between POA&amp;M Items for milestones

  ALIASES
    $ saf convert ckl2poam

  EXAMPLES
    $ saf convert ckl2POAM -i checklist_file.ckl -o output-folder -d abcdefg -s 2
</div></code></pre>
<p><a href="#other-useful-converters">top</a></p>
<hr>
<h3 id="emass-api-cli">eMASS API CLI</h3>
<p>The SAF CLI implements the eMASS REST API capabilities via the emasser CLI incorporated here with the SAF CLI. Please references the <a href="https://saf-cli.mitre.org/docs/emasser">emasser Features</a> 📜 for additional information</p>
<p>To get top level help execute the following commad:</p>
<pre class="hljs"><code><div>$ saf emasser [-h or -help]
[eMASS]        The eMASS REST API implementation

USAGE
  $ saf emasser COMMAND

TOPICS
  emasser delete  eMass REST API DELETE endpoint commands
  emasser get     eMass REST API GET endpoint commands
  emasser post    eMass REST API POST endpoint commands
  emasser put     eMass REST API PUT endpoint commands

COMMANDS
  emasser configure  Generate a configuration file (.env) for accessing an eMASS instances.
  emasser version    Display the eMASS API specification version the CLI implements.
</div></code></pre>
<p><a href="#emasser-client">top</a></p>
<hr>
<h3 id="view">View</h3>
<h4 id="heimdall">Heimdall</h4>
<p>You can start a local Heimdall Lite instance to visualize your findings with the SAF CLI. To start an instance use the <code>saf view heimdall</code> command:</p>
<pre class="hljs"><code><div>view heimdall                 Run an instance of Heimdall Lite to
                              visualize your data
  USAGE
    $ saf view heimdall [-h] [-p &lt;port&gt;] [-f &lt;file&gt;] [-n]

  FLAGS
    -f, --files=&lt;file&gt;...   File(s) to display in Heimdall
    -h, --help              Show CLI help.
    -n, --noOpenBrowser     Do not open the default browser automatically
    -p, --port=&lt;port&gt;       [default: 3000] Port To Expose Heimdall On (Default 3000)

  ALIASES
    $ saf heimdall

  EXAMPLES
    $ saf view heimdall -p 8080
</div></code></pre>
<p><a href="#view-hdf-summaries-and-data">top</a></p>
<h4 id="summary">Summary</h4>
<p>Generate a comprehensive summary of compliance data, including totals and counts, from your HDF files.</p>
<p>The output can be displayed in the console, or exported as YAML, JSON, or a GitHub-flavored Markdown table.</p>
<pre class="hljs"><code><div>USAGE
  <span class="hljs-variable">$</span> saf view summary <span class="hljs-literal">-i</span> &lt;value&gt; [-<span class="hljs-type">o</span> &lt;<span class="hljs-type">value</span>&gt;] [-<span class="hljs-type">f</span> <span class="hljs-type">json</span>|<span class="hljs-type">yaml</span>|<span class="hljs-type">markdown</span>] [-<span class="hljs-type">s</span>]
    [-<span class="hljs-type">r</span>] [-<span class="hljs-type">t</span>] [-<span class="hljs-type">l</span> &lt;<span class="hljs-type">value</span>&gt;] [-<span class="hljs-type">h</span>]

FLAGS
  <span class="hljs-literal">-h</span>, -<span class="hljs-literal">-help</span>  Show help information

FORMATTING FLAGS
  <span class="hljs-operator">-f</span>, -<span class="hljs-literal">-format</span>=&lt;option&gt;    [<span class="hljs-type">default</span>: <span class="hljs-type">yaml</span>] Specify output format
                           &lt;options: json|yaml|markdown&gt;
  <span class="hljs-literal">-r</span>, --[<span class="hljs-type">no</span>-]print<span class="hljs-literal">-pretty</span>  Enable human<span class="hljs-literal">-readable</span> <span class="hljs-keyword">data</span> output
  <span class="hljs-literal">-t</span>, --[<span class="hljs-type">no</span>-]title<span class="hljs-literal">-table</span>   Add titles to the markdown table(s)

I/O FLAGS
  <span class="hljs-literal">-i</span>, -<span class="hljs-literal">-input</span>=&lt;value&gt;...  (required) Specify input HDF file(s)
  <span class="hljs-literal">-o</span>, -<span class="hljs-literal">-output</span>=&lt;value&gt;    Specify output file(s)
  <span class="hljs-literal">-s</span>, --[<span class="hljs-type">no</span>-]stdout       Enable printing to console

DEBUGGING FLAGS
  <span class="hljs-literal">-l</span>, -<span class="hljs-literal">-logLevel</span>=&lt;value&gt;  [<span class="hljs-type">default</span>: <span class="hljs-type">info</span>] Set log level

DESCRIPTION
  Generate a comprehensive summary of compliance <span class="hljs-keyword">data</span>, including totals and
  counts, from your HDF files. The output can be displayed <span class="hljs-keyword">in</span> the console, or
  exported as YAML, JSON, or a GitHub<span class="hljs-literal">-flavored</span> Markdown table.

ALIASES
  <span class="hljs-variable">$</span> saf summary

EXAMPLES
  Basic Usage:

  Summarize <span class="hljs-string">'input.hdf'</span> single HDF file:

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.hdf

  Specify Formats:

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.json -<span class="hljs-literal">-format</span>=json

  Output GitHub Flavored Markdown Table, skip the console, and save to <span class="hljs-string">'output.md'</span>:

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.json -<span class="hljs-literal">-format</span>=markdown -<span class="hljs-literal">-no</span><span class="hljs-literal">-stdout</span> <span class="hljs-literal">-o</span> output.md

  Summarize multiple HDF files:

  <span class="hljs-variable">$</span> mycli summary -<span class="hljs-literal">-input</span> input1.hdf -<span class="hljs-literal">-input</span> input2.hdf

  The input (`-i`) flag also accepts a space delimited list of files:

  <span class="hljs-variable">$</span> mycli summary -<span class="hljs-literal">-input</span> input1.hdf input2.hdf

  Save summary to <span class="hljs-string">'output.json'</span> and the print to the console:

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.hdf -<span class="hljs-literal">-output</span> output.json

  Use short or long flag(s):

  <span class="hljs-variable">$</span> mycli summary -<span class="hljs-literal">-input</span> input.hdf -<span class="hljs-literal">-format</span> json

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.hdf <span class="hljs-operator">-f</span> yaml

    Formmated or RAW output:

  <span class="hljs-variable">$</span> mycli summary -<span class="hljs-literal">-input</span> input.hdf -<span class="hljs-literal">-pretty</span><span class="hljs-literal">-print</span> <span class="hljs-comment"># enable human-readable output</span>

  <span class="hljs-variable">$</span> mycli summary <span class="hljs-literal">-i</span> input.hdf -<span class="hljs-literal">-no</span><span class="hljs-literal">-pretty</span><span class="hljs-literal">-print</span>  <span class="hljs-comment"># for scripts or data-processing (RAW yaml/json/etc.)</span>
</div></code></pre>
<p><a href="#view-hdf-summaries-and-data">top</a></p>
<hr>
<h3 id="validate">Validate</h3>
<h4 id="thresholds">Thresholds</h4>
<p>See the wiki for more information on 👉 <a href="https://github.com/mitre/saf/wiki/Validation-with-Thresholds">template files</a>.</p>
<pre class="hljs"><code><div>validate threshold            Validate the compliance and status counts of an HDF file

  USAGE
    $ saf validate threshold -i &lt;hdf-json&gt; [-h] [-T &lt;flattened-threshold-json&gt; | -F &lt;template-file&gt;]

  FLAGS
    -F, --templateFile=&lt;template-file&gt;                Expected data template, generate one with &quot;saf generate threshold&quot;
    -T, --templateInline=&lt;flattened-threshold-json&gt;   Flattened JSON containing your validation thresholds
                                                      (Intended for backwards compatibility with InSpec Tools)
    -h, --help                                        Show CLI help.
    -i, --input=&lt;hdf-json&gt;                            (required) Input HDF JSON File

  EXAMPLES
    $ saf validate threshold -i rhel7-results.json -F output.yaml
</div></code></pre>
<p><a href="#validate-hdf-thresholds">top</a></p>
<hr>
<h3 id="generate">Generate</h3>
<h4 id="delta">Delta</h4>
<p>See the wiki for more information on 👉 <a href="https://github.com/mitre/saf/wiki/Delta-(WIP)">Delta</a>.</p>
<pre class="hljs"><code><div>Update an existing InSpec profile with updated XCCDF guidance

USAGE
  $ saf generate delta -J &lt;value&gt; -X &lt;value&gt; -o &lt;value&gt; [-h] [-O &lt;value&gt;] [-r &lt;value&gt;] [-T rule|group|cis|version] [-L info|warn|debug|verbose]

FLAGS
  -h, --help               Show CLI help.
  -J, --inspecJsonFile=&lt;value&gt;  (required) Input execution/profile JSON file - can be generated using the &quot;inspec json &lt;profile path&gt; | jq . &gt; profile.json&quot; command
  -X, --xccdfXmlFile=&lt;value&gt;    (required) The XCCDF XML file containing the new guidance - in the form of .xml file
  -o, --outputFolder=&lt;value&gt;    (required) The output folder for the updated profile - if not empty it will be overwritten
  -O, --ovalXmlFile=&lt;value&gt;     The OVAL XML file containing definitions used in the new guidance - in the form of .xml file
  -T, --idType=&lt;option&gt;         [default: rule] Control ID Types: 
                                'rule' - Vulnerability IDs (ex. 'SV-XXXXX'), 
                                'group' - Group IDs (ex. 'V-XXXXX'), 
                                'cis' - CIS Rule IDs (ex. C-1.1.1.1), 
                                'version' - Version IDs (ex. RHEL-07-010020 - also known as STIG IDs)
                                &lt;options: rule|group|cis|version&gt;
  -r, --report=&lt;value&gt;          Output markdown report file - must have an extension of .md
  -L, --logLevel=&lt;option&gt;       [default: info]
                                &lt;options: info|warn|debug|verbose&gt;

EXAMPLES
  $ saf generate delta -J ./the_profile_json_file.json -X ./the_xccdf_guidance_file.xml  -o the_output_directory -O ./the_oval_file.xml -T group -r the_update_report_file.md -L debug
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="delta-supporting-options">Delta Supporting Options</h4>
<p>Use this process prior of running <code>generate delta</code> if the updated guidance's have new control numbers and/or to format the controls how <code>generate delta</code> will. Running this process minimizes the delta output content and makes for better and easier visualization of the modification provided by the Delta process.</p>
<pre class="hljs"><code><div>USAGE
  $ saf generate update_controls4delta -X &lt;value&gt; -J &lt;value&gt; -c &lt;value&gt; [-P V|VS] [--[no-]backupControls] [--[no-]formatControls] [-L info|warn|debug|verbose]

FLAGS
  -h, --help                    Show CLI help.
  -X, --xccdfXmlFile=&lt;value&gt;    (required) The XCCDF XML file containing the new guidance - in the form of an .xml file
  -c, --controlsDir=&lt;value&gt;     (required) The InSpec profile controls directory containing the profiles to be updated  
  -J, --inspecJsonFile=&lt;value&gt;  Input execution/profile JSON file - can be generated using the &quot;inspec json &lt;profile path&gt; &gt; profile.json&quot; command. If not provided the `inspec` CLI must be installed
  -P, --controlPrefix=&lt;option&gt;  [default: V] Old control number prefix V or SV, default V &lt;options: V|SV&gt;
  -b, --[no-]backupControls     Preserve modified controls in a backup directory (oldControls) inside the controls directory
                                [default: true]
  -f, --[no-]formatControls     Format control contents in the same way `generate delta` will write controls
                                [default: true]
  -L, --logLevel=&lt;option&gt;       [default: info] &lt;options: info|warn|debug|verbose&gt; 

EXAMPLES
  $ saf generate update_controls4delta -X ./the_xccdf_guidance_file.xml -c the_controls_directory -L debug
  $ saf generate update_controls4delta -X ./the_xccdf_guidance_file.xml -J ./the_profile_json -c the_controls_directory -L debug
  $ saf generate update_controls4delta -X ./the_xccdf_guidance_file.xml -c the_controls_directory --no-formatControls -P SV -L debug
  $ saf generate update_controls4delta -X ./the_xccdf_guidance_file.xml -c the_controls_directory --no-backupControls --no-formatControls -P SV -L debug

</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="ckl-templates">CKL Templates</h4>
<p>Checklist template files are used to give extra information to <code>saf convert hdf2ckl</code>.</p>
<pre class="hljs"><code><div>generate ckl_metadata         Generate a checklist metadata template for &quot;saf convert hdf2ckl&quot;

  USAGE
    $ saf generate ckl_metadata -o &lt;json-file&gt; [-h]

  FLAGS
    -h, --help                Show CLI help.
    -o, --output=&lt;json-file&gt;  (required) Output JSON File

  EXAMPLES
    $ saf generate ckl_metadata -o rhel_metadata.json
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="inspec-metadata">InSpec Metadata</h4>
<p>InSpec metadata files are used to give extra information to <code>saf convert *2inspec_stub</code>.</p>
<pre class="hljs"><code><div>generate inspec_metadata      Generate an InSpec metadata template for &quot;saf convert *2inspec_stub&quot;

  USAGE
    $ saf generate inspec_metadata -o &lt;json-file&gt;

  FLAGS
    -h, --help                Show CLI help.
    -o, --output=&lt;json-file&gt;  (required) Output JSON File

  EXAMPLES
    $ saf generate inspec_metadata -o ms_sql_baseline_metadata.json
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="thresholds">Thresholds</h4>
<p>Threshold files are used in CI to ensure minimum compliance levels and validate control severities and statuses using <code>saf validate threshold</code></p>
<p>See the wiki for more information on 👉 <a href="https://github.com/mitre/saf/wiki/Validation-with-Thresholds">template files</a>.</p>
<pre class="hljs"><code><div>generate threshold            Generate a compliance template for &quot;saf validate threshold&quot;.
                              Default output states that you must have your current
                              control counts or better (More Passes and/or less
                              Fails/Skips/Not Applicable/No Impact/Errors)
  USAGE
    $ saf generate threshold -i &lt;hdf-json&gt; [-o &lt;threshold-yaml&gt;] [-h] [-e] [-c]

  FLAGS
    -c, --generateControlIds  Validate control IDs have the correct severity and status
    -e, --exact               All counts should be exactly the same when validating, not just less than or greater than
    -h, --help                Show CLI help.
    -i, --input=&lt;value&gt;       (required) Input HDF JSON File
    -o, --output=&lt;value&gt;      Output Threshold YAML File

  EXAMPLES
    $ saf generate threshold -i rhel7-results.json -e -c -o output.yaml
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="spreadsheet-csvxlsx-to-inspec">Spreadsheet (csv/xlsx) to InSpec</h4>
<p>You can use <code>saf generate spreadsheet2inspec_stub</code> to generate an InSpec profile stub from a spreadsheet file.</p>
<pre class="hljs"><code><div>generate spreadsheet2inspec_stub              Generate an InSpec profile stub from a CSV STIGs or CIS XLSX benchmarks

USAGE
  $ saf generate spreadsheet2inspec_stub -i, --input=&lt;XLSX or CSV&gt; -o, --output=FOLDER

OPTIONS
  -M, --mapping=mapping                      Path to a YAML file with mappings for each field, by default, CIS Benchmark
                                             fields are used for XLSX, STIG Viewer CSV export is used by CSV
  -c, --controlNamePrefix=controlNamePrefix  Prefix for all control IDs
  -f, --format=cis|disa|general              [default: general]
  -i, --input=input                          (required)
  -e, --encodingHeader                       Add the &quot;# encoding: UTF-8&quot; comment at the top of each control
  -l, --lineLength=lineLength                [default: 80] Characters between lines within InSpec controls
  -m, --metadata=metadata                    Path to a JSON file with additional metadata for the inspec.yml file
  -o, --output=output                        (required) [default: profile] Output InSpec profile stub folder


EXAMPLES
  saf generate spreadsheet2inspec_stub -i spreadsheet.xlsx -o profile
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="xccdf-benchmark-to-inspec-stub">XCCDF Benchmark to InSpec Stub</h4>
<pre class="hljs"><code><div>generate xccdf_benchmark2inspec_stub              Translate a DISA STIG XCCDF Benchmark XML file into a skeleton for an InSpec profile

USAGE
  $ saf generate xccdf_benchmark2inspec_stub -i &lt;stig-xccdf-xml&gt; [-o &lt;output-folder&gt;] [-h] [-m &lt;metadata-json&gt;] [-T (rule|group|cis|version)] [-s] [-L (info|warn|debug|verbose)]

FLAGS
  -h, --help                     Show CLI help.
  -i, --input=&lt;value&gt;            (required) Path to the XCCDF benchmark file
  -o, --output=&lt;value&gt;           [default: profile] The output folder to write the generated InSpec content
  -T, --idType=&lt;option&gt;          [default: rule] Control ID Types: 
                                 'rule' - Vulnerability IDs (ex. 'SV-XXXXX'), 
                                 'group' - Group IDs (ex. 'V-XXXXX'), 
                                 'cis' - CIS Rule IDs (ex. C-1.1.1.1), 
                                 'version' - Version IDs (ex. RHEL-07-010020 - also known as STIG IDs)
                                 &lt;options: rule|group|cis|version&gt;
  -O, --ovalDefinitions=&lt;value&gt;  Path to an OVAL definitions file to populate profile elements that reference OVAL defintions
  -m, --metadata=&lt;value&gt;         Path to a JSON file with additional metadata for the inspec.yml file
  -s, --singleFile               Output the resulting controls as a single file
  -L, --logLevel=&lt;option&gt;        [default: info] &lt;options: info|warn|debug|verbose&gt;

EXAMPLES
  $ saf generate xccdf_benchmark2inspec_stub -i ./U_RHEL_6_STIG_V2R2_Manual-xccdf.xml -T group --logLevel debug -r rhel-6-update-report.md
  $ saf generate xccdf_benchmark2inspec_stub -i ./CIS_Ubuntu_Linux_18.04_LTS_Benchmark_v1.1.0-xccdf.xml -O ./CIS_Ubuntu_Linux_18.04_LTS_Benchmark_v1.1.0-oval.xml --logLevel debug
</div></code></pre>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h5 id="dod-stub-vs-cis-stub-formatting">DoD Stub vs CIS Stub Formatting</h5>
<p>The converter supports both Stub and CIS styles. The <code>--format</code> flag is used to specify the required output format. Default is DoD Stub Format.</p>
<ul>
<li>Specifying the <code>--format</code> flag as either <code>cis</code> or <code>disa</code> will parse the input spreadsheet according to the standard formats for CIS Benchmark exports and DISA STIG exports, respectively.</li>
<li>You can also use the <code>general</code> setting (the default) to parse an arbitrary spreadsheet, but if you do so, you must provide a mapping file with the <code>--mapping</code> flag so that <code>saf</code> can parse the input.</li>
<li>If you provide a non-standard spreadsheet, the first row of values are assumed to be column headers.</li>
</ul>
<p><a href="#generate-data-reports-and-more">top</a></p>
<h4 id="mapping-files">Mapping Files</h4>
<p>Mapping files are YAML files that tell <code>saf</code> which columns in the input spreadsheet should be parsed. Mapping files are structured as following:</p>
<pre class="hljs"><code><div><span class="hljs-attr">id:</span>                           <span class="hljs-comment"># Required</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">ID</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">"recommendation #"</span>
<span class="hljs-attr">title:</span>                        <span class="hljs-comment"># Required</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Title</span>                     <span class="hljs-comment"># You can give more than one column header as a value for an</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">title</span>                     <span class="hljs-comment"># attribute if you are not sure how it will be spelled in the input.</span>
<span class="hljs-attr">desc:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Description</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Discussion</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">description</span>
<span class="hljs-attr">impact:</span> <span class="hljs-number">0.5</span>                  <span class="hljs-comment"># If impact is set, its value will be used for every control</span>
<span class="hljs-attr">desc.rationale:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Rationale</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">rationale</span> <span class="hljs-string">statement</span>
<span class="hljs-attr">desc.check:</span>                   <span class="hljs-comment"># Required</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Audit</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">audit</span> <span class="hljs-string">procedure</span>
<span class="hljs-attr">desc.fix:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Remediation</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">remediation</span> <span class="hljs-string">procedure</span>
<span class="hljs-attr">desc.additional_information:</span>  <span class="hljs-comment"># You can define arbitrary values under desc and tag</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Additional</span> <span class="hljs-string">Information</span>    <span class="hljs-comment"># if you have extra fields to record</span>
<span class="hljs-attr">desc.default_value:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">Default</span> <span class="hljs-string">Value</span>
<span class="hljs-attr">ref:</span>                          <span class="hljs-comment"># InSpec keyword - saf will check this column for URLs (links to documentation)</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">References</span>                <span class="hljs-comment"># and record each address as a ref attribute</span>
</div></code></pre>
<p>Where the keys (<code>title</code>) are InSpec control attributes and the values (<code>- Title</code>) are the column headers in the input spreadsheet that correspond to that attribute.</p>
<p><a href="#generate-data-reports-and-more">top</a></p>
<hr>
<h3 id="supplement">Supplement</h3>
<p>Supplement (ex. read or modify) elements that provide contextual information in an HDF file such as <code>passthrough</code> or <code>target</code></p>
<h4 id="passthrough">Passthrough</h4>
<p>Supplement (ex. read or modify) the <code>passthrough</code> element, which provides contextual information in the Heimdall Data Format results JSON file</p>
<pre class="hljs"><code><div>EXAMPLE (combined read, modfication, and overwrite of the original file)
  $ saf supplement passthrough read -i hdf_with_passthrough.json | jq -rc '.key = &quot;new value&quot;' | xargs -0 -I{} saf supplement passthrough write -i hdf_with_passthrough.json -d {}
</div></code></pre>
<p>Passthrough data can be any context/structure. See the sample below or visit 👉 <a href="https://github.com/mitre/saf/wiki/Supplement-HDF-files-with-additional-information-(ex.-%60passthrough%60,-%60target%60)">Supplement HDF files with additional information</a></p>
<pre class="hljs"><code><div>{
  <span class="hljs-attr">"CDM"</span>: {
    <span class="hljs-attr">"HWAM"</span>: {
      <span class="hljs-attr">"Asset_ID_Tattoo"</span>: <span class="hljs-string">"arn:aws:ec2:us-east-1:123456789012:instance/i-12345acbd5678efgh90"</span>,
      <span class="hljs-attr">"Data_Center_ID"</span>: <span class="hljs-string">"1234-5678-ABCD-1BB1-CC12DD34EE56FF78"</span>,
      <span class="hljs-attr">"FQDN"</span>: <span class="hljs-string">"i-12345acbd5678efgh90.ec2.internal"</span>,
      <span class="hljs-attr">"Hostname"</span>: <span class="hljs-string">"i-12345acbd5678efgh90"</span>,
      <span class="hljs-attr">"ipv4"</span>: <span class="hljs-string">"10.0.1.25"</span>,
      <span class="hljs-attr">"ipv6"</span>: <span class="hljs-string">"none defined"</span>,
      <span class="hljs-attr">"mac"</span>: <span class="hljs-string">"02:32:fd:e3:68:a1"</span>,
      <span class="hljs-attr">"os"</span>: <span class="hljs-string">"Linux"</span>,
      <span class="hljs-attr">"FISMA_ID"</span>: <span class="hljs-string">"ABCD2C21-7781-92AA-F126-FF987CZZZZ"</span>
    },
    <span class="hljs-attr">"CSM"</span>: {
      <span class="hljs-attr">"Server_Type"</span>: <span class="hljs-string">"member server"</span>,
      <span class="hljs-attr">"source_tool"</span>: <span class="hljs-string">"InSpec"</span>
    }
  }
}
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h5 id="read">Read</h5>
<pre class="hljs"><code><div>supplement passthrough read              Read the `passthrough` attribute in a given Heimdall Data Format JSON file and send it to stdout or write it to a file

USAGE
  $ saf supplement passthrough read -i &lt;hdf-json&gt; [-o &lt;passthrough-json&gt;]

FLAGS
  -h, --help            Show CLI help.
  -i, --input=&lt;value&gt;   (required) An input HDF file
  -o, --output=&lt;value&gt;  An output `passthrough` JSON file (otherwise the data is sent to stdout)

EXAMPLES
  $ saf supplement passthrough read -i hdf.json -o passthrough.json
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h5 id="write">Write</h5>
<pre class="hljs"><code><div>supplement passthrough write              Overwrite the `passthrough` attribute in a given HDF file with the provided `passthrough` JSON data

USAGE
  $ saf supplement passthrough write -i &lt;input-hdf-json&gt; (-f &lt;input-passthrough-json&gt; | -d &lt;passthrough-json&gt;) [-o &lt;output-hdf-json&gt;]

FLAGS
  -d, --passthroughData=&lt;value&gt;  Input passthrough-data (can be any valid JSON); this flag or `passthroughFile` must be provided
  -f, --passthroughFile=&lt;value&gt;  An input passthrough-data file (can contain any valid JSON); this flag or `passthroughData` must be provided
  -h, --help                     Show CLI help.
  -i, --input=&lt;value&gt;            (required) An input Heimdall Data Format file
  -o, --output=&lt;value&gt;           An output Heimdall Data Format JSON file (otherwise the input file is overwritten)

DESCRIPTION
  Passthrough data can be any context/structure. See sample ideas at [https://github.com/mitre/saf/wiki/Supplement-HDF-files-with-additional-information-(ex.-%60passthrough%60,-%60target%60)#:~:text=Settings-,Supplement%20HDF%20files%20with%20additional%20information,-(ex.%20%60passthrough%60%2C%20%60target](https://github.com/mitre/saf/wiki/Supplement-HDF-files-with-additional-information-(ex.-%60passthrough%60,-%60target%60))
  
EXAMPLES
  $ saf supplement passthrough write -i hdf.json -d '{&quot;a&quot;: 5}'

  $ saf supplement passthrough write -i hdf.json -f passthrough.json -o new-hdf.json
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h4 id="target">Target</h4>
<p>Supplement (ex. read or modify) the <code>target</code> element, which provides contextual information in the Heimdall Data Format results JSON file</p>
<pre class="hljs"><code><div>EXAMPLE (combined read, modfication, and overwrite of the original file)
  $ saf supplement target read -i hdf_with_target.json | jq -rc '.key = &quot;new value&quot;' | xargs -0 -I{} saf supplement target write -i hdf_with_target.json -d {}
</div></code></pre>
<p>Passthrough data can be any context/structure. See the sample below or visit 👉 <a href="https://github.com/mitre/saf/wiki/Supplement-HDF-files-with-additional-information-(ex.-%60passthrough%60,-%60target%60)">Supplement HDF files with additional information</a></p>
<pre class="hljs"><code><div>{
  <span class="hljs-attr">"AWS"</span>:{
    <span class="hljs-attr">"Resources"</span>:[
      {
        <span class="hljs-attr">"Type"</span>:<span class="hljs-string">"AwsEc2Instance"</span>,
        <span class="hljs-attr">"Id"</span>:<span class="hljs-string">"arn:aws:ec2:us-east-1:123456789012:instance/i-06036f0ccaa012345"</span>,
        <span class="hljs-attr">"Partition"</span>:<span class="hljs-string">"aws"</span>,
        <span class="hljs-attr">"Region"</span>:<span class="hljs-string">"us-east-1"</span>,
        <span class="hljs-attr">"Details"</span>:{
          <span class="hljs-attr">"AwsEc2Instance"</span>:{
            <span class="hljs-attr">"Type"</span>:<span class="hljs-string">"t2.medium"</span>,
            <span class="hljs-attr">"ImageId"</span>:<span class="hljs-string">"ami-0d716eddcc7b7abcd"</span>,
            <span class="hljs-attr">"IpV4Addresses"</span>:[
              <span class="hljs-string">"10.0.0.27"</span>
            ],
            <span class="hljs-attr">"KeyName"</span>:<span class="hljs-string">"rhel7_1_10152021"</span>,
            <span class="hljs-attr">"VpcId"</span>:<span class="hljs-string">"vpc-0b53ff8f37a06abcd"</span>,
            <span class="hljs-attr">"SubnetId"</span>:<span class="hljs-string">"subnet-0ea14519a4ddaabcd"</span>
          }
        }
      }
    ]
  }
}
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h5 id="read">Read</h5>
<pre class="hljs"><code><div>supplement target read              Read the `target` attribute in a given Heimdall Data Format JSON file and send it to stdout or write it to a file

USAGE
  $ saf supplement target read -i &lt;hdf-json&gt; [-o &lt;target-json&gt;]

FLAGS
  -h, --help            Show CLI help.
  -i, --input=&lt;value&gt;   (required) An input HDF file
  -o, --output=&lt;value&gt;  An output `target` JSON file (otherwise the data is sent to stdout)

EXAMPLES
  $ saf supplement target read -i hdf.json -o target.json
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h5 id="write">Write</h5>
<pre class="hljs"><code><div>supplement target write              Overwrite the `target` attribute in a given HDF file with the provided `target` JSON data

USAGE
  $ saf supplement target write -i &lt;input-hdf-json&gt; (-f &lt;input-target-json&gt; | -d &lt;target-json&gt;) [-o &lt;output-hdf-json&gt;]

FLAGS
  -d, --targetData=&lt;value&gt;  Input target-data (can be any valid JSON); this flag or `targetFile` must be provided
  -f, --targetFile=&lt;value&gt;  An input target-data file (can contain any valid JSON); this flag or `targetData` must be provided
  -h, --help                Show CLI help.
  -i, --input=&lt;value&gt;       (required) An input Heimdall Data Format file
  -o, --output=&lt;value&gt;      An output Heimdall Data Format JSON file (otherwise the input file is overwritten)

DESCRIPTION
  Target data can be any context/structure. See sample ideas at https://github.com/mitre/saf/wiki/Supplement-HDF-files-with-additional-information-(ex.-%60passthrough%60,-%60target%60)

EXAMPLES
  $ saf supplement target write -i hdf.json -d '{&quot;a&quot;: 5}'

  $ saf supplement target write -i hdf.json -f target.json -o new-hdf.json
</div></code></pre>
<p><a href="#enhance-and-supplement-hdf-data">top</a></p>
<h1 id="license-and-author">License and Author</h1>
<h3 id="authors">Authors</h3>
<ul>
<li>Author:: Will Dower <a href="https://github.com/wdower">wdower</a></li>
<li>Author:: Ryan Lin <a href="https://github.com/rlin232">Rlin232</a></li>
<li>Author:: Amndeep Singh Mann <a href="https://github.com/amndeep7">Amndeep7</a></li>
<li>Author:: Camden Moors <a href="https://github.com/camdenmoors">camdenmoors</a></li>
<li>Author:: Emily Rodriguez <a href="https://github.com/em-c-rod">em-c-rod</a></li>
<li>Author:: George Dias <a href="https://github.com/georgedias">georgedias</a></li>
</ul>
<h3 id="notice">NOTICE</h3>
<p>© 2022 The MITRE Corporation.</p>
<p>Approved for Public Release; Distribution Unlimited. Case Number 18-3678.</p>
<h3 id="notice">NOTICE</h3>
<p>MITRE hereby grants express written permission to use, reproduce, distribute, modify, and otherwise leverage this software to the extent permitted by the licensed terms provided in the LICENSE.md file included with this project.</p>
<h3 id="notice">NOTICE</h3>
<p>This software was produced for the U. S. Government under Contract Number HHSM-500-2012-00008I, and is subject to Federal Acquisition Regulation Clause 52.227-14, Rights in Data-General.</p>
<p>No other use other than that granted to the U. S. Government, or to those acting on behalf of the U. S. Government under that Clause is authorized without the express written permission of The MITRE Corporation.</p>
<p>For further information, please contact The MITRE Corporation, Contracts Management Office, 7515 Colshire Drive, McLean, VA 22102-7539, (703) 983-6000.</p>
<p><a href="#license-and-authors">top</a></p>

</body>
</html>
